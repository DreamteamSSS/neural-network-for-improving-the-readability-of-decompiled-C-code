{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8294705,"sourceType":"datasetVersion","datasetId":4927519},{"sourceId":8325381,"sourceType":"datasetVersion","datasetId":4945034}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This laptop was created to demonstrate the operation of an adapter for CodeLlama, designed to solve the problem of improving the readability of decompiled C code","metadata":{}},{"cell_type":"markdown","source":"# 1) Install the necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install accelerate\n!pip install bitsandbytes\n!pip install peft\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:28:30.788206Z","iopub.execute_input":"2024-05-21T19:28:30.788554Z","iopub.status.idle":"2024-05-21T19:29:27.488720Z","shell.execute_reply.started":"2024-05-21T19:28:30.788522Z","shell.execute_reply":"2024-05-21T19:29:27.487810Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2) Import block","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport os\nimport sys\nimport torch\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    prepare_model_for_kbit_training,\n    set_peft_model_state_dict,\n)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq, LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:30:22.480542Z","iopub.execute_input":"2024-05-21T19:30:22.480886Z","iopub.status.idle":"2024-05-21T19:30:43.982331Z","shell.execute_reply.started":"2024-05-21T19:30:22.480857Z","shell.execute_reply":"2024-05-21T19:30:43.981403Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-21 19:30:34.036878: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-21 19:30:34.036999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-21 19:30:34.191210: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3) Set device, basic model's tokenizer","metadata":{}},{"cell_type":"code","source":"# Set which device we will use (GPU or CPU).\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# The basic model\nBASE_MODEL = \"codellama/CodeLlama-7b-hf\"\n# Initializing the tokenizer\ntokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\ntokenizer.pad_token_id = 0\ntokenizer.padding_side = \"left\"","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:32:08.106832Z","iopub.execute_input":"2024-05-21T19:32:08.108175Z","iopub.status.idle":"2024-05-21T19:32:09.151164Z","shell.execute_reply.started":"2024-05-21T19:32:08.108125Z","shell.execute_reply":"2024-05-21T19:32:09.150386Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cade086c27a49c5a047164ad9d4c764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"648b984414424dc7886a4d54ed05090e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"620c489fb5524d24a5a1d547decdb914"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a97e569aa3b4b8789ee616008d2c996"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CodeLlamaTokenizer'. \nThe class this function is called from is 'LlamaTokenizer'.\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4) Upload our eval dataset with code examples (optional)","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\neval_dataset = load_dataset('csv', data_files='/kaggle/input/9000hr/9000hr (1).csv', split='train[0%:1%]')\nprint(len(eval_dataset))\na1 = '//FormAI DATASET v1.0 Category: Educational ; Style: Donald Knuth\\r\\n#include <stdio.h>\\r\\n\\r\\nint main() {\\r\\n  int n = 7;\\r\\n  int arr[n][n];\\r\\n\\r\\n  // filling the matrix\\r\\n  for(int i = 0; i < n; i++) {\\r\\n    for(int j = 0; j < n; j++) {\\r\\n      if(i == 0) {\\r\\n        arr[i][j] = j + 1;\\r\\n      } else if(j == 0) {\\r\\n        arr[i][j] = i + 1;\\r\\n      } else {\\r\\n        arr[i][j] = arr[i-1][j] + arr[i][j-1];\\r\\n      }\\r\\n    }\\r\\n  }\\r\\n\\r\\n  // printing the matrix\\r\\n  printf(\"The Pascal\\'s Triangle of order %d is:\\\\n\", n);\\r\\n  for(int i = 0; i < n; i++) {\\r\\n    for(int j = 0; j < n; j++) {\\r\\n      printf(\"%d\\\\t\", arr[i][j]);\\r\\n    }\\r\\n    printf(\"\\\\n\");\\r\\n  }\\r\\n\\r\\n  return 0;\\r\\n}'\na2 = '/* This file was generated by the Hex-Rays decompiler version 8.3.0.230608.\\r\\n   Copyright (c) 2007-2021 Hex-Rays <info@hex-rays.com>\\r\\n\\r\\n   Detected compiler: Visual C++\\r\\n*/\\r\\n\\r\\n#include <windows.h>\\r\\n#include <defs.h>\\r\\n\\r\\n\\r\\n//-------------------------------------------------------------------------\\r\\n// Function declarations\\r\\n\\r\\nint printf(const char *const Format, ...);\\r\\nint __fastcall main(int argc, const char **argv, const char **envp);\\r\\n__int64 __fastcall _main(_QWORD, _QWORD, _QWORD); // weak\\r\\n\\r\\n\\r\\n//----- (0000000140001591) ----------------------------------------------------\\r\\nint __fastcall main(int argc, const char **argv, const char **envp)\\r\\n{\\r\\n  void *v3; // rsp\\r\\n  int v4; // ecx\\r\\n  __int64 v5; // rdx\\r\\n  __int64 v7; // [rsp+0h] [rbp-80h] BYREF\\r\\n  char v8; // [rsp+20h] [rbp-60h] BYREF\\r\\n  __int64 *v9; // [rsp+28h] [rbp-58h]\\r\\n  __int64 v10; // [rsp+30h] [rbp-50h]\\r\\n  __int64 v11; // [rsp+38h] [rbp-48h]\\r\\n  __int64 v12; // [rsp+40h] [rbp-40h]\\r\\n  __int64 v13; // [rsp+48h] [rbp-38h]\\r\\n  char *v14; // [rsp+50h] [rbp-30h]\\r\\n  __int64 v15; // [rsp+58h] [rbp-28h]\\r\\n  __int64 v16; // [rsp+60h] [rbp-20h]\\r\\n  int v17; // [rsp+6Ch] [rbp-14h]\\r\\n  int m; // [rsp+70h] [rbp-10h]\\r\\n  int k; // [rsp+74h] [rbp-Ch]\\r\\n  int j; // [rsp+78h] [rbp-8h]\\r\\n  int i; // [rsp+7Ch] [rbp-4h]\\r\\n\\r\\n  _main(argc, argv, envp);\\r\\n  v9 = &v7;\\r\\n  v17 = 7;\\r\\n  v16 = 6i64;\\r\\n  v10 = 7i64;\\r\\n  v11 = 0i64;\\r\\n  v15 = 6i64;\\r\\n  v12 = 7i64;\\r\\n  v13 = 0i64;\\r\\n  v3 = alloca(208i64);\\r\\n  v14 = &v8;\\r\\n  for ( i = 0; i < v17; ++i )\\r\\n  {\\r\\n    for ( j = 0; j < v17; ++j )\\r\\n    {\\r\\n      if ( i )\\r\\n      {\\r\\n        if ( j )\\r\\n        {\\r\\n          v4 = *(_DWORD *)&v14[28 * i - 28 + 4 * j] + *(_DWORD *)&v14[28 * i - 4 + 4 * j];\\r\\n          v5 = j + 7i64 * i;\\r\\n        }\\r\\n        else\\r\\n        {\\r\\n          v4 = i + 1;\\r\\n          v5 = 7i64 * i;\\r\\n        }\\r\\n        *(_DWORD *)&v14[4 * v5] = v4;\\r\\n      }\\r\\n      else\\r\\n      {\\r\\n        *(_DWORD *)&v14[4 * j] = j + 1;\\r\\n      }\\r\\n    }\\r\\n  }\\r\\n  printf(\"The Pascal\\'s Triangle of order %d is:\\\\n\", (unsigned int)v17);\\r\\n  for ( k = 0; k < v17; ++k )\\r\\n  {\\r\\n    for ( m = 0; m < v17; ++m )\\r\\n      printf(\"%d\\\\t\", *(unsigned int *)&v14[28 * k + 4 * m]);\\r\\n    printf(\"\\\\n\");\\r\\n  }\\r\\n  return 0;\\r\\n}\\r\\n// 140001900: using guessed type __int64 __fastcall _main(_QWORD, _QWORD, _QWORD);\\r\\n\\r\\n// nfuncs=137 queued=1 decompiled=1 lumina nreq=0 worse=0 better=0\\r\\n// ALL OK, 1 function(s) have been successfully decompiled\\r\\n'","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:32:20.792794Z","iopub.execute_input":"2024-05-21T19:32:20.793175Z","iopub.status.idle":"2024-05-21T19:32:22.341130Z","shell.execute_reply.started":"2024-05-21T19:32:20.793145Z","shell.execute_reply":"2024-05-21T19:32:22.340219Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30691a60651e4457902e76ac454ace87"}},"metadata":{}},{"name":"stdout","text":"94\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example of decompiler output:\nprint(eval_dataset[0][a2])","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:52:26.151210Z","iopub.execute_input":"2024-05-21T19:52:26.152055Z","iopub.status.idle":"2024-05-21T19:52:26.157678Z","shell.execute_reply.started":"2024-05-21T19:52:26.152016Z","shell.execute_reply":"2024-05-21T19:52:26.156699Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"/* This file was generated by the Hex-Rays decompiler version 8.3.0.230608.\n   Copyright (c) 2007-2021 Hex-Rays <info@hex-rays.com>\n\n   Detected compiler: Visual C++\n*/\n\n#include <windows.h>\n#include <defs.h>\n\n\n//-------------------------------------------------------------------------\n// Function declarations\n\nint scanf(const char *const Format, ...);\nint printf(const char *const Format, ...);\nint __fastcall main(int argc, const char **argv, const char **envp);\n__int64 __fastcall _main(_QWORD, _QWORD, _QWORD); // weak\n\n\n//----- (00000001400015E2) ----------------------------------------------------\nint __fastcall main(int argc, const char **argv, const char **envp)\n{\n  void *v3; // rsp\n  int v5; // [rsp+20h] [rbp-20h] BYREF\n  int v6; // [rsp+24h] [rbp-1Ch] BYREF\n  int *v7; // [rsp+28h] [rbp-18h]\n  __int64 v8; // [rsp+30h] [rbp-10h]\n  int j; // [rsp+38h] [rbp-8h]\n  int i; // [rsp+3Ch] [rbp-4h]\n\n  _main(argc, argv, envp);\n  printf(\"Enter the number of terms you want to generate in the Fibonacci Sequence: \");\n  scanf(\"%d\", &v6);\n  v8 = v6 - 1i64;\n  v3 = alloca(16 * ((unsigned __int64)(4i64 * v6 + 15) >> 4));\n  v7 = &v5;\n  v5 = 0;\n  v6 = 1;\n  i = 2;\n  printf(\"\\nFibonacci Sequence for %d terms is: \", 1i64);\n  for ( i = 0; i < v6; ++i )\n    printf(\"%d \", (unsigned int)v7[i]);\n  printf(\"\\n\\nVisualizing the Fibonacci Sequence:\\n\");\n  for ( i = 0; i < v6; ++i )\n  {\n    for ( j = 0; j < v7[i]; ++j )\n      printf(\"* \");\n    printf(\"\\n\");\n  }\n  printf(\"\\n\");\n  return 0;\n}\n// 1400016CE: conditional instruction was optimized away because %var_34.4==2\n// 140001850: using guessed type __int64 __fastcall _main(_QWORD, _QWORD, _QWORD);\n\n// nfuncs=176 queued=1 decompiled=1 lumina nreq=0 worse=0 better=0\n// ALL OK, 1 function(s) have been successfully decompiled\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5) Download the model together with our checkpoint","metadata":{}},{"cell_type":"code","source":"model = LlamaForCausalLM.from_pretrained(\n    '/kaggle/input/explain-2400', #checkpoint folder\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type='nf4',\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:32:31.986248Z","iopub.execute_input":"2024-05-21T19:32:31.987032Z","iopub.status.idle":"2024-05-21T19:33:58.067297Z","shell.execute_reply.started":"2024-05-21T19:32:31.986974Z","shell.execute_reply":"2024-05-21T19:33:58.066444Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7bd5fc511094009bb202b4042762c5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d3d2b4069d846fcaf2e178431d91f37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"873d40d803514241bcb6f37f3d443d65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e16d2995aa2244148042558df9ab8e2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d10c9b70fdc54dd3891d516c853f943f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3799dbb847465299d5c922f37fa04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa86c1fdf2b4d06a63dbc7d9b147b8f"}},"metadata":{}}]},{"cell_type":"code","source":"# Architecture of the model with an adapter\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:34:31.686972Z","iopub.execute_input":"2024-05-21T19:34:31.687707Z","iopub.status.idle":"2024-05-21T19:34:31.703721Z","shell.execute_reply.started":"2024-05-21T19:34:31.687673Z","shell.execute_reply":"2024-05-21T19:34:31.702710Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32016, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=1, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=1, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=1, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=1, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=1, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=1, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=1, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=1, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Additional preparation of the model for use\nfrom peft import PeftModel\nmodel = PeftModel.from_pretrained(model, '/kaggle/input/explain-2400')\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0 \nmodel.config.bos_token_id = 1\nmodel.config.eos_token_id = 2","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:35:04.036027Z","iopub.execute_input":"2024-05-21T19:35:04.036885Z","iopub.status.idle":"2024-05-21T19:35:04.242429Z","shell.execute_reply.started":"2024-05-21T19:35:04.036852Z","shell.execute_reply":"2024-05-21T19:35:04.241675Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 6) Evaluation","metadata":{}},{"cell_type":"code","source":"eval_prompt = \"\"\"You are a powerful decompiler model. Your job is to convert the С code decompiled using Hex-Rays decompiler into a more human-readable form. That is, you should change the names of variables and functions and delete unnecessary parts of the code so that it looks more like the source code of a C program. You are given a С code decompiled using Hex-Rays decompiler.\n\nYou must output the source code of a C program.\n\n### Your input:\n/* This file was generated by the Hex-Rays decompiler version 8.3.0.230608.\n   Copyright (c) 2007-2021 Hex-Rays <info@hex-rays.com>\n\n   Detected compiler: Visual C++\n*/\n\n#include <windows.h>\n#include <defs.h>\n\n\n//-------------------------------------------------------------------------\n// Function declarations\n\nint scanf(const char *const Format, ...);\nint printf(const char *const Format, ...);\nint __fastcall main(int argc, const char **argv, const char **envp);\n__int64 __fastcall _main(_QWORD, _QWORD, _QWORD); // weak\n\n\n//----- (00000001400015E2) ----------------------------------------------------\nint __fastcall main(int argc, const char **argv, const char **envp)\n{\n  void *v3; // rsp\n  int v5; // [rsp+20h] [rbp-20h] BYREF\n  int v6; // [rsp+24h] [rbp-1Ch] BYREF\n  int *v7; // [rsp+28h] [rbp-18h]\n  __int64 v8; // [rsp+30h] [rbp-10h]\n  int j; // [rsp+38h] [rbp-8h]\n  int i; // [rsp+3Ch] [rbp-4h]\n\n  _main(argc, argv, envp);\n  printf(\"Enter the number of terms you want to generate in the Fibonacci Sequence: \");\n  scanf(\"%d\", &v6);\n  v8 = v6 - 1i64;\n  v3 = alloca(16 * ((unsigned __int64)(4i64 * v6 + 15) >> 4));\n  v7 = &v5;\n  v5 = 0;\n  v6 = 1;\n  i = 2;\n  printf(\"\\nFibonacci Sequence for %d terms is: \", 1i64);\n  for ( i = 0; i < v6; ++i )\n    printf(\"%d \", (unsigned int)v7[i]);\n  printf(\"\\n\\nVisualizing the Fibonacci Sequence:\\n\");\n  for ( i = 0; i < v6; ++i )\n  {\n    for ( j = 0; j < v7[i]; ++j )\n      printf(\"* \");\n    printf(\"\\n\");\n  }\n  printf(\"\\n\");\n  return 0;\n}\n// 1400016CE: conditional instruction was optimized away because %var_34.4==2\n// 140001850: using guessed type __int64 __fastcall _main(_QWORD, _QWORD, _QWORD);\n\n// nfuncs=176 queued=1 decompiled=1 lumina nreq=0 worse=0 better=0\n// ALL OK, 1 function(s) have been successfully decompiled\n\n### The original C program:\n\"\"\"\n\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=500)[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:36:34.127579Z","iopub.execute_input":"2024-05-21T19:36:34.128210Z","iopub.status.idle":"2024-05-21T19:37:32.592441Z","shell.execute_reply.started":"2024-05-21T19:36:34.128178Z","shell.execute_reply":"2024-05-21T19:37:32.591257Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You are a powerful decompiler model. Your job is to convert the С code decompiled using Hex-Rays decompiler into a more human-readable form. That is, you should change the names of variables and functions and delete unnecessary parts of the code so that it looks more like the source code of a C program. You are given a С code decompiled using Hex-Rays decompiler.\n\nYou must output the source code of a C program.\n\n### Your input:\n/* This file was generated by the Hex-Rays decompiler version 8.3.0.230608.\n   Copyright (c) 2007-2021 Hex-Rays <info@hex-rays.com>\n\n   Detected compiler: Visual C++\n*/\n\n#include <windows.h>\n#include <defs.h>\n\n\n//-------------------------------------------------------------------------\n// Function declarations\n\nint scanf(const char *const Format, ...);\nint printf(const char *const Format, ...);\nint __fastcall main(int argc, const char **argv, const char **envp);\n__int64 __fastcall _main(_QWORD, _QWORD, _QWORD); // weak\n\n\n//----- (00000001400015E2) ----------------------------------------------------\nint __fastcall main(int argc, const char **argv, const char **envp)\n{\n  void *v3; // rsp\n  int v5; // [rsp+20h] [rbp-20h] BYREF\n  int v6; // [rsp+24h] [rbp-1Ch] BYREF\n  int *v7; // [rsp+28h] [rbp-18h]\n  __int64 v8; // [rsp+30h] [rbp-10h]\n  int j; // [rsp+38h] [rbp-8h]\n  int i; // [rsp+3Ch] [rbp-4h]\n\n  _main(argc, argv, envp);\n  printf(\"Enter the number of terms you want to generate in the Fibonacci Sequence: \");\n  scanf(\"%d\", &v6);\n  v8 = v6 - 1i64;\n  v3 = alloca(16 * ((unsigned __int64)(4i64 * v6 + 15) >> 4));\n  v7 = &v5;\n  v5 = 0;\n  v6 = 1;\n  i = 2;\n  printf(\"\nFibonacci Sequence for %d terms is: \", 1i64);\n  for ( i = 0; i < v6; ++i )\n    printf(\"%d \", (unsigned int)v7[i]);\n  printf(\"\n\nVisualizing the Fibonacci Sequence:\n\");\n  for ( i = 0; i < v6; ++i )\n  {\n    for ( j = 0; j < v7[i]; ++j )\n      printf(\"* \");\n    printf(\"\n\");\n  }\n  printf(\"\n\");\n  return 0;\n}\n// 1400016CE: conditional instruction was optimized away because %var_34.4==2\n// 140001850: using guessed type __int64 __fastcall _main(_QWORD, _QWORD, _QWORD);\n\n// nfuncs=176 queued=1 decompiled=1 lumina nreq=0 worse=0 better=0\n// ALL OK, 1 function(s) have been successfully decompiled\n\n### The original C program:\n//FormAI DATASET v1.0 Category: Fibonacci Sequence Visualizer ; Style: ultraprecise\n#include <stdio.h>\n\nint main() {\n    int n, i, j, fib[100], count = 0;\n    printf(\"Enter the number of terms you want to generate in the Fibonacci Sequence: \");\n    scanf(\"%d\", &n);\n\n    fib[0] = 0;\n    fib[1] = 1;\n    for (i = 2; i < n; i++) {\n        fib[i] = fib[i-1] + fib[i-2];\n    }\n\n    printf(\"Fibonacci Sequence for %d terms is: \", n);\n    for (i = 0; i < n; i++) {\n        printf(\"%d \", fib[i]);\n    }\n    printf(\"\\n\\n\");\n\n    printf(\"Visualizing the Fibonacci Sequence:\\n\");\n    for (i = 0; i < n; i++) {\n        for (j = 0; j < fib[i]; j++) {\n            printf(\"* \");\n        }\n        printf(\"\\n\");\n    }\n    printf(\"\\n\");\n\n    return 0;\n}\n\n// The output will be something like this:\n// Enter the number of terms you want to generate in the Fibonacci Sequence: 10\n// Fibonacci Sequence for 10 terms is: 0 1 1 2 3 5 8 13 21 34\n//\n// Visualizing the Fibonacci Sequence:\n// *\n// * *\n// * * *\n// * * * *\n// * * * * *\n// * * * * * *\n// * * * * * * *\n// * * * * * * * *\n// * * * * * * * * *\n// * * * * * * * * * *\n// * * * * * * * * * * *\n// * * * * * * * * * * * *\n// * * * * * * * * * * * * *\n//\n","output_type":"stream"}]}]}