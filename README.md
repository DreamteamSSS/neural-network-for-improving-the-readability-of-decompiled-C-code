# Исследование возможностей файн-тюнинга LLM для решения задачи повышения читабельности декомпилированного кода на языке Си
В большинстве случаев декомпилированный программный код трудно поддается анализу: названия переменных и функций лишены изначального заложенного смысла и трудно прослеживается логика программы. В ходе работы над проектом был обучен адаптер для языковой модели CodeLlama, предназначенный для улучшения декомпилированного кода на языке Си: приближения к исходному коду программы и упрощения для человеческого восприятия. Также исследованы возможности адаптера и проведена оценка его эффективности при решении данной задачи.
В качестве LLM была выбрана модель CodeLlama-7b [4] – дообученная модель Llama 2 для написания, завершения и исправления кода. Обучение адаптера с 1,05 млн. обучаемых параметров (0,015 % от всех параметров модели) проводилось на основании датасета, состоящего из 176 тыс. примеров из исходного кода на Си (часть из которых взята из датасета FormAI Dataset) и соответствующего результата работы декомпилятора Hex-Rays (версия - 8.3.0.230608; компилятор – GCC 11.4.0). 
Проведенные тесты показывают, что модель с адаптером, несмотря на относительно небольшие для NLP объем датасета и количество эпох, достигает приличных результатов, в том числе при обработке кода, декомпилированного при помощи программ, примеров вывода которых не было в обучающей выборке (например, RetDec). Corpus BLEU = 0.41, Sentence BLEU = 0.32. В дальнейшие планы работы над проектом входят: продолжение обучения модели на датасете большего объема и с примерами работы других декомпиляторов, проведение более масштабного тестирования как с классическими для задачи seq2seq метриками (BLEU, AED и т.д.), так и с оценкой при помощи опроса специалистов и с проверкой возможности перекомпилирования результатов работы нейросети.
