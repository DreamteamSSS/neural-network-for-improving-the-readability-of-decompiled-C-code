{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6678053,"sourceType":"datasetVersion","datasetId":3852781}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[torch]\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4DMFTMcvA9F","outputId":"1943b234-d810-4829-96e9-1b8f6064aafc","execution":{"iopub.status.busy":"2023-11-09T15:40:02.046990Z","iopub.execute_input":"2023-11-09T15:40:02.048249Z","iopub.status.idle":"2023-11-09T15:40:18.735465Z","shell.execute_reply.started":"2023-11-09T15:40:02.048207Z","shell.execute_reply":"2023-11-09T15:40:18.733563Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0+cpu)\nRequirement already satisfied: accelerate>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.22.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport transformers\nfrom transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\nmodel = transformers.OpenAIGPTLMHeadModel(transformers.OpenAIGPTConfig(n_positions=3000, vocab_size=40479, n_layers=1))","metadata":{"execution":{"iopub.status.busy":"2023-11-09T15:40:18.738076Z","iopub.execute_input":"2023-11-09T15:40:18.739312Z","iopub.status.idle":"2023-11-09T15:40:41.139868Z","shell.execute_reply.started":"2023-11-09T15:40:18.739264Z","shell.execute_reply":"2023-11-09T15:40:41.138535Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"035269aba25b4f9f81fd2a66527246be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc545369b71f4dbf9de7da3671c4a3c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e033b80daf24f9ca47f5c1f0cd78b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719cb5a8e8794a9eb274b4a39b037d40"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\npath=\"\"\ndata=pd.read_csv(\"/kaggle/input/1111111/data.csv\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEy-x-JgJ_-B","outputId":"5cca66d1-caf1-4415-dd72-60b1af67188b","execution":{"iopub.status.busy":"2023-11-09T15:40:41.141594Z","iopub.execute_input":"2023-11-09T15:40:41.142047Z","iopub.status.idle":"2023-11-09T15:40:41.536854Z","shell.execute_reply.started":"2023-11-09T15:40:41.142006Z","shell.execute_reply":"2023-11-09T15:40:41.535527Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data[\"0\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-18T18:37:15.321703Z","iopub.execute_input":"2023-10-18T18:37:15.322068Z","iopub.status.idle":"2023-10-18T18:37:15.328010Z","shell.execute_reply.started":"2023-10-18T18:37:15.322036Z","shell.execute_reply":"2023-10-18T18:37:15.326947Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'#include<bits/stdc++.h> \\n using namespace std;\\n\\nconst int chushu=13;\\nint main()\\n{\\nchar n[101],shang[100];\\nint a,i;\\nscanf(\"%s\",n);\\nif (n[1]==\\'\\\\0\\') {printf (\"0\\\\n%s\",n);}\\nelse if (n[0]==\\'1\\' && n[1]<\\'3\\' && n[2]==\\'\\\\0\\') {printf (\"0\\\\n%s\",n);}\\nelse\\n{\\n  a=n[0]-\\'0\\';\\n  a=a*10+n[1]-\\'0\\';\\n  if(a<13)\\n  {\\n  for(i=1;n[i+1]!=\\'\\\\0\\';i++)\\n      {\\n      a=a*10+n[i+1]-\\'0\\';\\n      shang[i-1]=a/chushu+\\'0\\';\\n      a=a%chushu;\\n      }\\n  shang[i-1]=\\'\\\\0\\';\\n  }\\n  else\\n  {\\n  shang[0]=a/chushu+\\'0\\';\\n  a=a%chushu;\\n  for(i=1;n[i+1]!=\\'\\\\0\\';i++)\\n      {\\n      a=a*10+n[i+1]-\\'0\\';\\n      shang[i]=a/chushu+\\'0\\';\\n      a=a%chushu;\\n      }\\n  shang[i]=\\'\\\\0\\';\\n  }\\nprintf(\"%s\\\\n%d\",shang,a);\\n}\\nreturn 0;\\n}'"},"metadata":{}}]},{"cell_type":"markdown","source":"'#include<bits/stdc++.h> \\n using namespace std;\\n\\nconst int chushu=13;\\nint main()\\n{\\nchar n[101],shang[100];\\nint a,i;\\nscanf(\"%s\",n);\\nif (n[1]==\\'\\\\0\\') {printf (\"0\\\\n%s\",n);}\\nelse if (n[0]==\\'1\\' && n[1]<\\'3\\' && n[2]==\\'\\\\0\\') {printf (\"0\\\\n%s\",n);}\\nelse\\n{\\n  a=n[0]-\\'0\\';\\n  a=a*10+n[1]-\\'0\\';\\n  if(a<13)\\n  {\\n  for(i=1;n[i+1]!=\\'\\\\0\\';i++)\\n      {\\n      a=a*10+n[i+1]-\\'0\\';\\n      shang[i-1]=a/chushu+\\'0\\';\\n      a=a%chushu;\\n      }\\n  shang[i-1]=\\'\\\\0\\';\\n  }\\n  else\\n  {\\n  shang[0]=a/chushu+\\'0\\';\\n  a=a%chushu;\\n  for(i=1;n[i+1]!=\\'\\\\0\\';i++)\\n      {\\n      a=a*10+n[i+1]-\\'0\\';\\n      shang[i]=a/chushu+\\'0\\';\\n      a=a%chushu;\\n      }\\n  shang[i]=\\'\\\\0\\';\\n  }\\nprintf(\"%s\\\\n%d\",shang,a);\\n}\\nreturn 0;\\n}'","metadata":{}},{"cell_type":"code","source":"data[\"1\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-18T18:37:54.648311Z","iopub.execute_input":"2023-10-18T18:37:54.649235Z","iopub.status.idle":"2023-10-18T18:37:54.656170Z","shell.execute_reply.started":"2023-10-18T18:37:54.649200Z","shell.execute_reply":"2023-10-18T18:37:54.654752Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'//\\n// This file was generated by the Retargetable Decompiler\\n// Website: https://retdec.com\\n//\\n\\n#include <stdint.h>\\n#include <stdio.h>\\n#include <stdlib.h>\\n\\n// ------------------- Function Prototypes --------------------\\n\\nint64_t __do_global_dtors_aux(void);\\nint64_t _fini(void);\\nint64_t _GLOBAL__sub_I_main(void);\\nint64_t _init(void);\\nint64_t _start(int64_t a1, int64_t a2, int64_t a3, int64_t a4, int64_t a5, int64_t a6);\\nint64_t _Z41__static_initialization_and_destruction_0ii(int32_t a1, int32_t a2);\\nint64_t deregister_tm_clones(void);\\nint64_t frame_dummy(void);\\nvoid function_1080(int64_t * d);\\nint32_t function_1090(char * format, ...);\\nint32_t function_10a0(void (*func)(int64_t *), int64_t * arg, int64_t * dso_handle);\\nvoid function_10b0(void);\\nint32_t function_10c0(char * format, ...);\\nint64_t function_10d0(void);\\nint64_t register_tm_clones(void);\\n\\n// --------------------- Global Variables ---------------------\\n\\nint64_t g1 = 0; // 0x3fd0\\nint64_t g2; // 0x4008\\nint64_t g3 = 0; // 0x4010\\nint64_t g4; // 0x4011\\nint32_t g5;\\n\\n// ------- Dynamically Linked Functions Without Header --------\\n\\nint32_t __cxa_atexit(void (*a1)(int64_t *), int64_t * a2, int64_t * a3);\\nvoid __cxa_finalize(int64_t * a1);\\nvoid __gmon_start__(void);\\nint32_t __libc_start_main(int64_t a1, int32_t a2, char ** a3, void (*a4)(), void (*a5)(), void (*a6)());\\nvoid __stack_chk_fail(void);\\nint64_t _ZNSt8ios_base4InitC1Ev(void);\\n\\n// ------------------------ Functions -------------------------\\n\\n// Address range: 0x1000 - 0x101b\\nint64_t _init(void) {\\n    int64_t result = 0; // 0x1012\\n    if (*(int64_t *)0x3fe8 != 0) {\\n        // 0x1014\\n        __gmon_start__();\\n        result = &g5;\\n    }\\n    // 0x1016\\n    return result;\\n}\\n\\n// Address range: 0x1080 - 0x108b\\nvoid function_1080(int64_t * d) {\\n    // 0x1080\\n    __cxa_finalize(d);\\n}\\n\\n// Address range: 0x1090 - 0x109b\\nint32_t function_1090(char * format, ...) {\\n    // 0x1090\\n    return printf(format);\\n}\\n\\n// Address range: 0x10a0 - 0x10ab\\nint32_t function_10a0(void (*func)(int64_t *), int64_t * arg, int64_t * dso_handle) {\\n    // 0x10a0\\n    return __cxa_atexit(func, arg, dso_handle);\\n}\\n\\n// Address range: 0x10b0 - 0x10bb\\nvoid function_10b0(void) {\\n    // 0x10b0\\n    __stack_chk_fail();\\n}\\n\\n// Address range: 0x10c0 - 0x10cb\\nint32_t function_10c0(char * format, ...) {\\n    // 0x10c0\\n    return scanf(format);\\n}\\n\\n// Address range: 0x10d0 - 0x10db\\nint64_t function_10d0(void) {\\n    // 0x10d0\\n    return _ZNSt8ios_base4InitC1Ev();\\n}\\n\\n// Address range: 0x10e0 - 0x1106\\nint64_t _start(int64_t a1, int64_t a2, int64_t a3, int64_t a4, int64_t a5, int64_t a6) {\\n    // 0x10e0\\n    int64_t v1; // 0x10e0\\n    __libc_start_main(0x11c9, (int32_t)a6, (char **)&v1, NULL, NULL, (void (*)())a3);\\n    __asm_hlt();\\n    // UNREACHABLE\\n}\\n\\n// Address range: 0x1110 - 0x1139\\nint64_t deregister_tm_clones(void) {\\n    // 0x1110\\n    return &g3;\\n}\\n\\n// Address range: 0x1140 - 0x1179\\nint64_t register_tm_clones(void) {\\n    // 0x1140\\n    return 0;\\n}\\n\\n// Address range: 0x1180 - 0x11b9\\nint64_t __do_global_dtors_aux(void) {\\n    // 0x1180\\n    if (*(char *)&g3 != 0) {\\n        // 0x11b8\\n        int64_t result; // 0x1180\\n        return result;\\n    }\\n    // 0x118d\\n    if (g1 != 0) {\\n        // 0x119b\\n        __cxa_finalize((int64_t *)*(int64_t *)0x4008);\\n    }\\n    int64_t result2 = deregister_tm_clones(); // 0x11a7\\n    *(char *)&g3 = 1;\\n    return result2;\\n}\\n\\n// Address range: 0x11c0 - 0x11c9\\nint64_t frame_dummy(void) {\\n    // 0x11c0\\n    return register_tm_clones();\\n}\\n\\n// Address range: 0x11c9 - 0x14ee\\nint main(int argc, char ** argv) {\\n    int64_t v1 = __readfsqword(40); // 0x11d8\\n    int32_t v2; // bp-120, 0x11c9\\n    scanf(\"%s\", &v2);\\n    int32_t v3; // 0x11c9\\n    char v4 = v3; // 0x1206\\n    if (v4 != 0) {\\n        // 0x122a\\n        char v5; // 0x11c9\\n        if (v4 > 50 || v5 == 0 != ((char)v2 == 49)) {\\n            // 0x1262\\n            int64_t v6; // bp-8, 0x11c9\\n            int64_t v7 = &v6; // 0x11ce\\n            int32_t v8 = (0x1000000 * v3 >> 24) - 528 + 10 * (0x1000000 * v2 >> 24); // 0x128c\\n            char v9; // bp-232, 0x11c9\\n            int32_t v10; // 0x11c9\\n            if (v8 > 12) {\\n                uint64_t v11 = 0x4ec4ec4f * (int64_t)v8; // 0x1388\\n                v9 = (char)(v11 / 0x400000000) + 48;\\n                int32_t v12 = -13 * ((int32_t)(v11 / 0x100000000) >> 2) + v8; // 0x13d3\\n                char v13 = *(char *)(v7 - 110); // 0x1492\\n                int32_t v14 = v12; // 0x1499\\n                int32_t v15 = 1; // 0x1499\\n                if (v13 != 0) {\\n                    int32_t v16 = 2;\\n                    int32_t v17 = ((int32_t)v13 - 48 + 10 * v12) % 13;\\n                    int32_t v18 = v16 + 1; // 0x148d\\n                    char v19 = *(char *)(v7 - 112 + (int64_t)v18); // 0x1492\\n                    int32_t v20 = v17; // 0x1499\\n                    v14 = v17;\\n                    v15 = v16;\\n                    while (v19 != 0) {\\n                        // 0x13ea\\n                        v16 = v18;\\n                        v17 = ((int32_t)v19 - 48 + 10 * v20) % 13;\\n                        v18 = v16 + 1;\\n                        v19 = *(char *)(v7 - 112 + (int64_t)v18);\\n                        v20 = v17;\\n                        v14 = v17;\\n                        v15 = v16;\\n                    }\\n                }\\n                // 0x149f\\n                *(char *)(v7 - 224 + (int64_t)v15) = 0;\\n                v10 = v14;\\n            } else {\\n                char v21 = *(char *)(v7 - 110); // 0x135a\\n                int32_t v22 = v8; // 0x1361\\n                int32_t v23 = 1; // 0x1361\\n                if (v21 != 0) {\\n                    int32_t v24 = 2;\\n                    int32_t v25 = ((int32_t)v21 - 48 + 10 * v8) % 13;\\n                    int32_t v26 = v24 + 1; // 0x1355\\n                    char v27 = *(char *)(v7 - 112 + (int64_t)v26); // 0x135a\\n                    int32_t v28 = v25; // 0x1361\\n                    v22 = v25;\\n                    v23 = v24;\\n                    while (v27 != 0) {\\n                        // 0x12b1\\n                        v24 = v26;\\n                        v25 = ((int32_t)v27 - 48 + 10 * v28) % 13;\\n                        v26 = v24 + 1;\\n                        v27 = *(char *)(v7 - 112 + (int64_t)v26);\\n                        v28 = v25;\\n                        v22 = v25;\\n                        v23 = v24;\\n                    }\\n                }\\n                // 0x1367\\n                *(char *)(v7 - 224 + (int64_t)(v23 - 1)) = 0;\\n                v10 = v22;\\n            }\\n            // 0x14af\\n            printf(\"%s\\\\n%d\", &v9, (int64_t)v10);\\n        } else {\\n            // 0x1242\\n            printf(\"0\\\\n%s\", &v2);\\n        }\\n    } else {\\n        // 0x120a\\n        printf(\"0\\\\n%s\", &v2);\\n    }\\n    int64_t result = 0; // 0x14e5\\n    if (v1 != __readfsqword(40)) {\\n        // 0x14e7\\n        __stack_chk_fail();\\n        result = &g5;\\n    }\\n    // 0x14ec\\n    return result;\\n}\\n\\n// Address range: 0x14ee - 0x1544\\n// Demangled:     __static_initialization_and_destruction_0(int, int)\\nint64_t _Z41__static_initialization_and_destruction_0ii(int32_t a1, int32_t a2) {\\n    int64_t result; // 0x14ee\\n    if (a1 == 1 == a2 == 0xffff) {\\n        // 0x150f\\n        _ZNSt8ios_base4InitC1Ev();\\n        int32_t v1 = __cxa_atexit((void (*)(int64_t *))*(int64_t *)0x3ff8, &g4, &g2); // 0x153c\\n        result = v1;\\n    }\\n    // 0x1541\\n    return result;\\n}\\n\\n// Address range: 0x1544 - 0x155d\\nint64_t _GLOBAL__sub_I_main(void) {\\n    // 0x1544\\n    return _Z41__static_initialization_and_destruction_0ii(1, 0xffff);\\n}\\n\\n// Address range: 0x1560 - 0x156d\\nint64_t _fini(void) {\\n    // 0x1560\\n    int64_t result; // 0x1560\\n    return result;\\n}\\n\\n// --------------------- Meta-Information ---------------------\\n\\n// Detected compiler/packer: gcc (11.4.0)\\n// Detected functions: 16\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.model_max_length=3000","metadata":{"id":"ByijYwfSu8Ht","execution":{"iopub.status.busy":"2023-11-09T15:40:41.539835Z","iopub.execute_input":"2023-11-09T15:40:41.540232Z","iopub.status.idle":"2023-11-09T15:40:41.547452Z","shell.execute_reply.started":"2023-11-09T15:40:41.540196Z","shell.execute_reply":"2023-11-09T15:40:41.546232Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer(data[\"1\"][0])","metadata":{"execution":{"iopub.status.busy":"2023-10-18T18:38:37.167599Z","iopub.execute_input":"2023-10-18T18:38:37.168139Z","iopub.status.idle":"2023-10-18T18:38:37.191772Z","shell.execute_reply.started":"2023-10-18T18:38:37.168092Z","shell.execute_reply":"2023-10-18T18:38:37.190762Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3208 > 3000). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [278, 278, 278, 278, 616, 4135, 509, 17031, 702, 481, 492, 14511, 1155, 585, 1047, 6697, 278, 278, 12757, 271, 28168, 271, 278, 278, 13242, 34651, 239, 5259, 278, 278, 285, 9990, 295, 495, 1693, 241, 239, 242, 290, 285, 9990, 295, 495, 19870, 239, 242, 290, 285, 9990, 295, 495, 10, 519, 259, 239, 242, 290, 278, 278, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 9161, 10750, 9060, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 2016, 27457, 279, 241, 279, 279, 587, 279, 14175, 279, 10, 3827, 279, 31466, 276, 9559, 275, 270, 2016, 27457, 279, 241, 279, 894, 249, 276, 9559, 275, 270, 2016, 27457, 279, 241, 279, 14175, 279, 279, 11147, 279, 249, 279, 2921, 276, 9559, 275, 270, 2016, 27457, 279, 241, 279, 478, 507, 276, 9559, 275, 270, 2016, 27457, 279, 241, 279, 1759, 276, 2016, 27457, 279, 241, 8, 277, 240, 2016, 27457, 279, 241, 8, 280, 240, 2016, 27457, 279, 241, 8, 281, 240, 2016, 27457, 279, 241, 8, 282, 240, 2016, 27457, 279, 241, 8, 284, 240, 2016, 27457, 279, 241, 8, 287, 275, 270, 2016, 27457, 279, 241, 279, 36, 20558, 279, 279, 11696, 279, 4881, 24396, 279, 488, 279, 7716, 279, 48, 7735, 276, 2016, 14814, 279, 241, 8, 277, 240, 2016, 14814, 279, 241, 8, 280, 275, 270, 2016, 27457, 279, 241, 771, 5, 743, 1089, 279, 3, 258, 279, 23858, 276, 9559, 275, 270, 2016, 27457, 279, 241, 4317, 279, 19770, 276, 9559, 275, 270, 9559, 9161, 279, 8582, 17164, 276, 2016, 27457, 279, 241, 269, 248, 275, 270, 2016, 14814, 279, 241, 9161, 279, 8582, 18711, 276, 21780, 269, 34034, 240, 239, 239, 239, 275, 270, 2016, 14814, 279, 241, 9161, 279, 8582, 8, 286, 276, 9559, 276, 269, 2049, 263, 275, 276, 2016, 27457, 279, 241, 269, 275, 240, 2016, 27457, 279, 241, 269, 489, 268, 240, 2016, 27457, 279, 241, 269, 10, 620, 279, 2861, 275, 270, 9559, 9161, 279, 8582, 21, 286, 276, 9559, 275, 270, 2016, 14814, 279, 241, 9161, 279, 8582, 25, 286, 276, 21780, 269, 34034, 240, 239, 239, 239, 275, 270, 2016, 27457, 279, 241, 9161, 279, 8582, 10, 286, 276, 9559, 275, 270, 2016, 27457, 279, 241, 8737, 279, 3, 258, 279, 23858, 276, 9559, 275, 270, 278, 278, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 14175, 35316, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 2016, 27457, 279, 241, 30, 277, 303, 286, 270, 278, 278, 48, 34, 43, 12, 10, 286, 2016, 27457, 279, 241, 30, 280, 270, 278, 278, 48, 34, 44, 12354, 292, 2016, 27457, 279, 241, 30, 281, 303, 286, 270, 278, 278, 48, 34, 24744, 5895, 2016, 27457, 279, 241, 30, 282, 270, 278, 278, 48, 34, 24744, 6682, 2016, 14814, 279, 241, 30, 284, 270, 278, 278, 260, 260, 260, 260, 260, 260, 260, 13905, 23052, 9224, 16015, 1147, 662, 732, 260, 260, 260, 260, 260, 260, 260, 260, 2016, 14814, 279, 241, 279, 279, 25, 34, 246, 279, 517, 5073, 276, 9559, 276, 269, 8, 277, 275, 276, 2016, 27457, 279, 241, 269, 275, 240, 2016, 27457, 279, 241, 269, 8, 280, 240, 2016, 27457, 279, 241, 269, 8, 281, 275, 270, 9559, 279, 279, 25, 34, 246, 279, 894, 10504, 276, 2016, 27457, 279, 241, 269, 8, 277, 275, 270, 9559, 279, 279, 30, 2035, 279, 1759, 279, 279, 276, 9559, 275, 270, 2016, 14814, 279, 241, 279, 279, 519, 21692, 279, 1759, 279, 2921, 276, 2016, 27457, 279, 241, 8, 277, 240, 2016, 14814, 279, 241, 8, 280, 240, 21780, 269, 269, 8, 281, 240, 9559, 276, 269, 8, 282, 275, 276, 275, 240, 9559, 276, 269, 8, 284, 275, 276, 275, 240, 9559, 276, 269, 8, 287, 275, 276, 275, 275, 270, 9559, 279, 279, 6855, 279, 573, 265, 279, 7209, 276, 9559, 275, 270, 2016, 27457, 279, 241, 279, 36, 9, 495, 54, 11, 2616, 279, 7362, 44, 478, 502, 25, 39, 17611, 276, 9559, 275, 270, 278, 278, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 16015, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 278, 278, 5039, 5855, 271, 48, 34, 33469, 260, 48, 34, 8582, 39, 259, 2016, 27457, 279, 241, 279, 478, 507, 276, 9559, 275, 309, 2016, 27457, 279, 241, 5718, 303, 286, 270, 278, 278, 48, 34, 8582, 6642, 645, 276, 269, 276, 2016, 27457, 279, 241, 269, 275, 48, 34, 43, 629, 292, 267, 303, 286, 275, 309, 278, 278, 48, 34, 8582, 7765, 279, 279, 30, 2035, 279, 1759, 279, 279, 276, 275, 270, 5718, 303, 296, 30, 284, 270, 310, 278, 278, 48, 34, 8582, 9509, 2294, 5718, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 8582, 17164, 260, 48, 34, 8582, 54, 259, 9559, 9161, 279, 8582, 17164, 276, 2016, 27457, 279, 241, 269, 248, 275, 309, 278, 278, 48, 34, 8582, 17164, 279, 279, 25, 34, 246, 279, 894, 10504, 276, 248, 275, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 8582, 18711, 260, 48, 34, 8582, 53, 259, 2016, 14814, 279, 241, 9161, 279, 8582, 18711, 276, 21780, 269, 34034, 240, 239, 239, 239, 275, 309, 278, 278, 48, 34, 8582, 18711, 2294, 28260, 250, 276, 34034, 275, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 8582, 8, 286, 260, 48, 34, 8582, 17444, 2016, 14814, 279, 241, 9161, 279, 8582, 8, 286, 276, 9559, 276, 269, 2049, 263, 275, 276, 2016, 27457, 279, 241, 269, 275, 240, 2016, 27457, 279, 241, 269, 489, 268, 240, 2016, 27457, 279, 241, 269, 10, 620, 279, 2861, 275, 309, 278, 278, 48, 34, 8582, 8, 286, 2294, 279, 279, 25, 34, 246, 279, 517, 5073, 276, 2049, 263, 240, 489, 268, 240, 10, 620, 279, 2861, 275, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 8582, 21, 286, 260, 48, 34, 8582, 17404, 9559, 9161, 279, 8582, 21, 286, 276, 9559, 275, 309, 278, 278, 48, 34, 8582, 21, 286, 279, 279, 6855, 279, 573, 265, 279, 7209, 276, 275, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 8582, 25, 286, 260, 48, 34, 8582, 25, 259, 2016, 14814, 279, 241, 9161, 279, 8582, 25, 286, 276, 21780, 269, 34034, 240, 239, 239, 239, 275, 309, 278, 278, 48, 34, 8582, 25, 286, 2294, 3971, 250, 276, 34034, 275, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 8582, 10, 286, 260, 48, 34, 8582, 10, 259, 2016, 27457, 279, 241, 9161, 279, 8582, 10, 286, 276, 9559, 275, 309, 278, 278, 48, 34, 8582, 10, 286, 2294, 279, 36, 9, 495, 54, 11, 2616, 279, 7362, 44, 478, 502, 25, 39, 17611, 276, 275, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 8582, 5, 286, 260, 48, 34, 39, 8582, 287, 2016, 27457, 279, 241, 279, 1759, 276, 2016, 27457, 279, 241, 8, 277, 240, 2016, 27457, 279, 241, 8, 280, 240, 2016, 27457, 279, 241, 8, 281, 240, 2016, 27457, 279, 241, 8, 282, 240, 2016, 27457, 279, 241, 8, 284, 240, 2016, 27457, 279, 241, 8, 287, 275, 309, 278, 278, 48, 34, 8582, 5, 286, 2016, 27457, 279, 241, 23, 277, 270, 278, 278, 48, 34, 8582, 5, 286, 279, 279, 519, 21692, 279, 1759, 279, 2921, 276, 48, 34, 13726, 25, 291, 240, 276, 2016, 14814, 279, 241, 275, 8, 287, 240, 276, 21780, 269, 269, 275, 296, 23, 277, 240, 33837, 240, 33837, 240, 276, 9559, 276, 269, 275, 276, 275, 275, 8, 281, 275, 270, 279, 279, 4822, 279, 4, 23229, 276, 275, 270, 278, 278, 40229, 310, 278, 278, 5039, 5855, 271, 48, 34, 13726, 5895, 260, 48, 34, 13726, 19755, 2016, 27457, 279, 241, 771, 5, 743, 1089, 279, 3, 258, 279, 23858, 276, 9559, 275, 309, 278, 278, 48, 34, 13726, 5895, 2294, 296, 30, 281, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 13726, 11083, 260, 48, 34, 39, 12568, 291, 2016, 27457, 279, 241, 8737, 279, 3, 258, 279, 23858, 276, 9559, 275, 309, 278, 278, 48, 34, 13726, 11083, 2294, 286, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 39, 30713, 260, 48, 34, 13726, 21, 291, 2016, 27457, 279, 241, 279, 279, 587, 279, 14175, 279, 10, 3827, 279, 31466, 276, 9559, 275, 309, 278, 278, 48, 34, 39, 30713, 645, 276, 269, 276, 21780, 269, 275, 296, 30, 281, 267, 303, 286, 275, 309, 278, 278, 48, 34, 13726, 21, 292, 2016, 27457, 279, 241, 5718, 270, 278, 278, 48, 34, 39, 30713, 2294, 5718, 270, 310, 278, 278, 48, 34, 39, 8083, 248, 645, 276, 30, 277, 267, 303, 286, 275, 309, 278, 278, 48, 34, 39, 3814, 259, 279, 279, 25, 34, 246, 279, 894, 10504, 276, 276, 2016, 27457, 279, 241, 269, 275, 269, 276, 2016, 27457, 279, 241, 269, 275, 48, 34, 44, 12354, 292, 275, 270, 310, 2016, 27457, 279, 241, 3785, 3, 280, 303, 771, 5, 743, 1089, 279, 3, 258, 279, 23858, 276, 275, 270, 278, 278, 48, 34, 13726, 8, 288, 269, 276, 21780, 269, 275, 296, 30, 281, 303, 277, 270, 2294, 3785, 3, 280, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 13726, 25, 286, 260, 48, 34, 13726, 25, 291, 2016, 27457, 279, 241, 4317, 279, 19770, 276, 9559, 275, 309, 278, 278, 48, 34, 13726, 25, 286, 2294, 8737, 279, 3, 258, 279, 23858, 276, 275, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 13726, 25, 291, 260, 48, 34, 15582, 10286, 1786, 2921, 276, 1786, 34276, 263, 240, 21780, 269, 269, 34276, 261, 275, 309, 2016, 27457, 279, 241, 23, 277, 303, 279, 279, 8320, 12, 14, 35, 1392, 276, 11083, 275, 270, 278, 278, 48, 34, 13726, 10, 292, 2016, 14814, 279, 241, 23, 280, 270, 278, 278, 21, 262, 260, 32548, 240, 48, 34, 13726, 25, 291, 3971, 250, 276, 244, 297, 252, 244, 240, 296, 23, 280, 275, 270, 2016, 14814, 279, 241, 23, 281, 270, 278, 278, 48, 34, 13726, 25, 291, 21780, 23, 282, 303, 23, 281, 270, 278, 278, 48, 34, 39, 4804, 287, 645, 276, 23, 282, 267, 303, 286, 275, 309, 278, 278, 48, 34, 13103, 42, 246, 21780, 23, 284, 270, 278, 278, 48, 34, 13726, 25, 291, 645, 276, 23, 282, 290, 8393, 323, 323, 23, 284, 303, 303, 286, 267, 303, 276, 276, 21780, 275, 23, 280, 303, 303, 23443, 275, 275, 309, 278, 278, 48, 34, 13103, 29656, 2016, 27457, 279, 241, 23, 287, 270, 278, 278, 21, 262, 260, 292, 240, 48, 34, 13726, 25, 291, 2016, 27457, 279, 241, 23, 288, 303, 296, 23, 287, 270, 278, 278, 48, 34, 13726, 608, 2016, 14814, 279, 241, 23, 292, 303, 276, 48, 34, 8582, 12354, 7397, 269, 23, 281, 290, 290, 10361, 275, 260, 46, 14423, 306, 5895, 269, 276, 48, 34, 8582, 12354, 7397, 269, 23, 280, 290, 290, 10361, 275, 270, 278, 278, 48, 34, 13103, 54, 263, 21780, 23, 291, 270, 278, 278, 21, 262, 260, 42, 14814, 240, 48, 34, 13726, 25, 291, 2016, 14814, 279, 241, 23, 5895, 270, 278, 278, 48, 34, 13726, 25, 291, 645, 276, 23, 292, 290, 6642, 275, 309, 16, 2016, 27457, 279, 241, 23, 6682, 303, 48, 34, 44, 713, 44, 713, 44, 250, 269, 276, 2016, 27457, 279, 241, 275, 23, 292, 270, 278, 278, 48, 34, 16670, 24781, 23, 291, 303, 276, 21780, 275, 276, 23, 6682, 278, 48, 34, 44, 12354, 33669, 7397, 275, 306, 20986, 270, 2016, 14814, 279, 241, 23, 6642, 303, 260, 7877, 269, 276, 276, 2016, 14814, 279, 241, 275, 276, 23, 6682, 278, 48, 34, 8582, 12354, 12354, 7397, 275, 290, 290, 280, 275, 306, 23, 292, 270, 278, 278, 48, 34, 16670, 10, 281, 21780, 23, 7877, 303, 269, 276, 21780, 269, 275, 276, 23, 288, 260, 39, 5895, 275, 270, 278, 278, 48, 34, 15582, 32964, 2016, 14814, 279, 241, 23, 7765, 303, 23, 6642, 270, 278, 278, 48, 34, 15582, 21106, 2016, 14814, 279, 241, 23, 7648, 303, 277, 270, 278, 278, 48, 34, 15582, 21106, 645, 276, 23, 7877, 267, 303, 286, 275, 309, 2016, 14814, 279, 241, 23, 9509, 303, 280, 270, 2016, 14814, 279, 241, 23, 9756, 303, 276, 276, 2016, 14814, 279, 241, 275, 23, 7877, 260, 20986, 306, 5895, 269, 23, 6642, 275, 297, 7877, 270, 2016, 14814, 279, 241, 23, 9982, 303, 23, 9509, 306, 277, 270, 278, 278, 48, 34, 15582, 54, 248, 21780, 23, 11252, 303, 269, 276, 21780, 269, 275, 276, 23, 288, 260, 39, 6642, 306, 276, 2016, 27457, 279, 241, 275, 23, 9982, 275, 270, 278, 278, 48, 34, 15582, 32964, 2016, 14814, 279, 241, 23, 7379, 303, 23, 9756, 270, 278, 278, 48, 34, 15582, 21106, 23, 7765, 303, 23, 9756, 270, 23, 7648, 303, 23, 9509, 270, 1000, 276, 23, 11252, 267, 303, 286, 275, 309, 278, 278, 48, 34, 16670, 18573, 23, 9509, 303, 23, 9982, 270, 23, 9756, 303, 276, 276, 2016, 14814, 279, 241, 275, 23, 11252, 260, 20986, 306, 5895, 269, 23, 7379, 275, 297, 7877, 270, 23, 9982, 303, 23, 9509, 306, 277, 270, 23, 11252, 303, 269, 276, 21780, 269, 275, 276, 23, 288, 260, 39, 6642, 306, 276, 2016, 27457, 279, 241, 275, 23, 9982, 275, 270, 23, 7379, 303, 23, 9756, 270, 23, 7765, 303, 23, 9756, 270, 23, 7648, 303, 23, 9509, 270, 310, 310, 278, 278, 48, 34, 15582, 53, 250, 269, 276, 21780, 269, 275, 276, 23, 288, 260, 42, 10361, 306, 276, 2016, 27457, 279, 241, 275, 23, 7648, 275, 303, 286, 270, 23, 5895, 303, 23, 7765, 270, 310, 1284, 309, 21780, 23, 11043, 303, 269, 276, 21780, 269, 275, 276, 23, 288, 260, 39, 5895, 275, 270, 278, 278, 48, 34, 16670, 46, 246, 2016, 14814, 279, 241, 23, 10393, 303, 23, 292, 270, 278, 278, 48, 34, 16670, 32132, 2016, 14814, 279, 241, 23, 11768, 303, 277, 270, 278, 278, 48, 34, 16670, 32132, 645, 276, 23, 11043, 267, 303, 286, 275, 309, 2016, 14814, 279, 241, 23, 10361, 303, 280, 270, 2016, 14814, 279, 241, 23, 10353, 303, 276, 276, 2016, 14814, 279, 241, 275, 23, 11043, 260, 20986, 306, 5895, 269, 23, 292, 275, 297, 7877, 270, 2016, 14814, 279, 241, 23, 13963, 303, 23, 10361, 306, 277, 270, 278, 278, 48, 34, 16670, 20218, 21780, 23, 13609, 303, 269, 276, 21780, 269, 275, 276, 23, 288, 260, 39, 6642, 306, 276, 2016, 27457, 279, 241, 275, 23, 13963, 275, 270, 278, 278, 48, 34, 16670, 46, 246, 2016, 14814, 279, 241, 23, 14423, 303, 23, 10353, 270, 278, 278, 48, 34, 16670, 32132, 23, 10393, 303, 23, 10353, 270, 23, 11768, 303, 23, 10361, 270, 1000, 276, 23, 13609, 267, 303, 286, 275, 309, 278, 278, 48, 34, 13103, 21, 277, 23, 10361, 303, 23, 13963, 270, 23, 10353, 303, 276, 276, 2016, 14814, 279, 241, 275, 23, 13609, 260, 20986, 306, 5895, 269, 23, 14423, 275, 297, 7877, 270, 23, 13963, 303, 23, 10361, 306, 277, 270, 23, 13609, 303, 269, 276, 21780, 269, 275, 276, 23, 288, 260, 39, 6642, 306, 276, 2016, 27457, 279, 241, 275, 23, 13963, 275, 270, 23, 14423, 303, 23, 10353, 270, 23, 10393, 303, 23, 10353, 270, 23, 11768, 303, 23, 10361, 270, 310, 310, 278, 278, 48, 34, 16670, 28888, 269, 276, 21780, 269, 275, 276, 23, 288, 260, 42, 10361, 306, 276, 2016, 27457, 279, 241, 275, 276, 23, 11768, 260, 277, 275, 275, 303, 286, 270, 23, 5895, 303, 23, 10393, 270, 310, 278, 278, 48, 34, 15582, 8, 250, 28260, 250, 276, 244, 297, 252, 365, 247, 297, 248, 244, 240, 296, 23, 291, 240, 276, 2016, 27457, 279, 241, 275, 23, 5895, 275, 270, 310, 1284, 309, 278, 278, 48, 34, 13103, 18905, 28260, 250, 276, 244, 286, 365, 247, 297, 252, 244, 240, 296, 23, 280, 275, 270, 310, 310, 1284, 309, 278, 278, 48, 34, 39, 4804, 246, 28260, 250, 276, 244, 286, 365, 247, 297, 252, 244, 240, 296, 23, 280, 275, 270, 310, 2016, 27457, 279, 241, 5718, 303, 286, 270, 278, 278, 48, 34, 15582, 5, 284, 645, 276, 23, 277, 267, 303, 279, 279, 8320, 12, 14, 35, 1392, 276, 11083, 275, 275, 309, 278, 278, 48, 34, 15582, 5, 288, 279, 279, 6855, 279, 573, 265, 279, 7209, 276, 275, 270, 5718, 303, 296, 30, 284, 270, 310, 278, 278, 48, 34, 15582, 28172, 2294, 5718, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 15582, 10286, 260, 48, 34, 14792, 18837, 278, 278, 3257, 7804, 271, 279, 279, 11696, 279, 4881, 24396, 279, 488, 279, 7716, 279, 286, 276, 1786, 240, 1786, 275, 2016, 27457, 279, 241, 279, 36, 20558, 279, 279, 11696, 279, 4881, 24396, 279, 488, 279, 7716, 279, 48, 7735, 276, 2016, 14814, 279, 241, 8, 277, 240, 2016, 14814, 279, 241, 8, 280, 275, 309, 2016, 27457, 279, 241, 5718, 270, 278, 278, 48, 34, 15582, 10286, 645, 276, 8, 277, 303, 303, 277, 303, 303, 8, 280, 303, 303, 48, 34, 919, 1410, 275, 309, 278, 278, 48, 34, 14792, 48, 250, 279, 36, 9, 495, 54, 11, 2616, 279, 7362, 44, 478, 502, 25, 39, 17611, 276, 275, 270, 2016, 14814, 279, 241, 23, 277, 303, 279, 279, 25, 34, 246, 279, 517, 5073, 276, 276, 9559, 276, 269, 275, 276, 2016, 27457, 279, 241, 269, 275, 275, 269, 276, 2016, 27457, 279, 241, 269, 275, 48, 34, 43, 919, 292, 240, 296, 30, 282, 240, 296, 30, 280, 275, 270, 278, 278, 48, 34, 14792, 43, 263, 5718, 303, 23, 277, 270, 310, 278, 278, 48, 34, 14792, 20558, 2294, 5718, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 14792, 18837, 260, 48, 34, 14792, 46, 248, 2016, 27457, 279, 241, 279, 14175, 279, 279, 11147, 279, 249, 279, 2921, 276, 9559, 275, 309, 278, 278, 48, 34, 14792, 18837, 2294, 279, 36, 20558, 279, 279, 11696, 279, 4881, 24396, 279, 488, 279, 7716, 279, 48, 7735, 276, 277, 240, 48, 34, 919, 1410, 275, 270, 310, 278, 278, 5039, 5855, 271, 48, 34, 14792, 15936, 260, 48, 34, 14792, 49, 248, 2016, 27457, 279, 241, 279, 894, 249, 276, 9559, 275, 309, 278, 278, 48, 34, 14792, 15936, 2016, 27457, 279, 241, 5718, 270, 278, 278, 48, 34, 14792, 15936, 2294, 5718, 270, 310, 278, 278, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 744, 2338, 260, 2444, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 278, 278, 11314, 1047, 6697, 278, 4909, 483, 271, 30, 21750, 276, 6682, 239, 282, 239, 286, 275, 278, 278, 11314, 16015, 271, 9509], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=10)","metadata":{"id":"xLED5BIRGpDb","execution":{"iopub.status.busy":"2023-11-09T15:40:41.548875Z","iopub.execute_input":"2023-11-09T15:40:41.549290Z","iopub.status.idle":"2023-11-09T15:40:42.688054Z","shell.execute_reply.started":"2023-11-09T15:40:41.549259Z","shell.execute_reply":"2023-11-09T15:40:42.687069Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport glob\nclass CDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n      self.data=data\n\n\n\n    def __getitem__(self, idx):\n        t={'input_ids': torch.tensor(tokenizer(self.data[\"1\"][idx], padding='max_length', truncation=True).input_ids), 'attention_mask': torch.tensor(tokenizer(self.data[\"1\"][idx], padding='max_length', truncation=True).attention_mask), 'labels': torch.tensor(tokenizer(self.data[\"0\"][idx], padding='max_length', truncation=True).input_ids)}\n        #item=dict()\n        #item['labels'] = torch.tensor(tokenizer(self.data[\"0\"][idx], return_tensors=\"pt\", padding=\"max_length\").input_ids).clone().detach()\n        #item[\"input\"] = torch.tensor(tokenizer(self.data[\"1\"][idx], return_tensors=\"pt\", padding=\"max_length\").input_ids).clone().detach()\n        return t\n\n    def __len__(self):\n        return len(self.data)\n\ntrain_dataset = CDataset(data)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"KTCnewYgGA3d","execution":{"iopub.status.busy":"2023-11-09T15:40:42.689581Z","iopub.execute_input":"2023-11-09T15:40:42.690547Z","iopub.status.idle":"2023-11-09T15:40:42.705945Z","shell.execute_reply.started":"2023-11-09T15:40:42.690492Z","shell.execute_reply":"2023-11-09T15:40:42.704452Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Using eos_token, but it is not set yet.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.add_special_tokens({'pad_token': '[PAD]'})","metadata":{"execution":{"iopub.status.busy":"2023-11-09T15:40:42.707642Z","iopub.execute_input":"2023-11-09T15:40:42.708593Z","iopub.status.idle":"2023-11-09T15:40:42.727642Z","shell.execute_reply.started":"2023-11-09T15:40:42.708549Z","shell.execute_reply":"2023-11-09T15:40:42.726409Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",          # output directory\n    num_train_epochs=1,              # total number of training epochs\n    per_device_train_batch_size=1,  # batch size per device during training\n    per_device_eval_batch_size=1,   # batch size for evaluation\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir=\"/kaggle/working/\", \n    gradient_accumulation_steps=1,# directory for storing logs\n    logging_steps=1,\n    prediction_loss_only=True \n)\n\n\ntrainer = Trainer(\n    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n)\n\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCb2SavOF1IN","outputId":"85c2bc3f-4995-436b-95ac-c6e87897a199","execution":{"iopub.status.busy":"2023-11-09T15:40:46.160760Z","iopub.execute_input":"2023-11-09T15:40:46.161229Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231109_154113-knuf2vny</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mephists/huggingface/runs/knuf2vny' target=\"_blank\">twilight-feather-24</a></strong> to <a href='https://wandb.ai/mephists/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mephists/huggingface' target=\"_blank\">https://wandb.ai/mephists/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mephists/huggingface/runs/knuf2vny' target=\"_blank\">https://wandb.ai/mephists/huggingface/runs/knuf2vny</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='31' max='1130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  31/1130 35:57 < 22:42:38, 0.01 it/s, Epoch 0.03/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>11.174000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>7.190700</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.739500</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.881100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.387700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.504000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.782800</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.862400</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.562800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.599300</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.987400</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.249800</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.971900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.608300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.775900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.095100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>3.059400</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.848600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.848900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.162200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.987600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.966400</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.994000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.187900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.368100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.845900</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.381100</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.708400</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.226600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.add_special_tokens({'pad_token': '[PAD]'})","metadata":{"id":"JZRba4y8F2gC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = (\n    \"Lets do our work\"\n)\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\ngen_tokens = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.9,\n    max_length=100,\n)\ngen_text = tokenizer.batch_decode(input_ids)[0]\nprint(gen_text)","metadata":{"id":"CaZcvpYDFdr9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"aw2lU6IMKUF-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.max_length=100000","metadata":{"id":"m69ImipnP36I","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = (\n    \"Lets do our work by this way\"\n)\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")\ninput_ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeNCkKhbIeKy","outputId":"3fb703e3-4781-40e1-f5f6-b910fb6aa9f4","trusted":true},"execution_count":null,"outputs":[]}]}